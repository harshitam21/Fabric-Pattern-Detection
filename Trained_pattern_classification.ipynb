{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e3217a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk space: 162148 MB free\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7842f25309e46689e237c2a1520c581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10365d778d904d42934d84b071476390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4751, 'grad_norm': 73.02481079101562, 'learning_rate': 4.171666666666667e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb32348f8b94a12a356c217bd3adf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2580868005752563, 'eval_accuracy': 0.6933333333333334, 'eval_runtime': 39.442, 'eval_samples_per_second': 30.424, 'eval_steps_per_second': 3.803, 'epoch': 1.0}\n",
      "{'loss': 1.6705, 'grad_norm': 73.44835662841797, 'learning_rate': 3.3383333333333333e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd664d78c4be4fa188935e42df35b3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.999728798866272, 'eval_accuracy': 0.735, 'eval_runtime': 71.233, 'eval_samples_per_second': 16.846, 'eval_steps_per_second': 2.106, 'epoch': 2.0}\n",
      "{'loss': 1.4107, 'grad_norm': 141.67831420898438, 'learning_rate': 2.505e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7e246cb0984fe3ba4f8c25c115154a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.946610689163208, 'eval_accuracy': 0.745, 'eval_runtime': 37.342, 'eval_samples_per_second': 32.135, 'eval_steps_per_second': 4.017, 'epoch': 3.0}\n",
      "{'loss': 1.2268, 'grad_norm': 82.1813735961914, 'learning_rate': 1.6716666666666666e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cf83b23ffc47c9971b11c23f93e9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8811412453651428, 'eval_accuracy': 0.7625, 'eval_runtime': 36.7005, 'eval_samples_per_second': 32.697, 'eval_steps_per_second': 4.087, 'epoch': 4.0}\n",
      "{'loss': 1.1091, 'grad_norm': 80.7503433227539, 'learning_rate': 8.400000000000001e-07, 'epoch': 4.17}\n",
      "{'loss': 1.0241, 'grad_norm': 125.87419891357422, 'learning_rate': 6.666666666666667e-09, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91eecfebb16546d49bccacb294f8b9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8599340915679932, 'eval_accuracy': 0.7691666666666667, 'eval_runtime': 46.0972, 'eval_samples_per_second': 26.032, 'eval_steps_per_second': 3.254, 'epoch': 5.0}\n",
      "{'train_runtime': 2164.284, 'train_samples_per_second': 11.084, 'train_steps_per_second': 1.386, 'train_loss': 1.4860526326497396, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b6083508d94c949ca897f5c07eb817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "   abstract_geometric_fabric       0.70      0.67      0.69        49\n",
      "               argyle_fabric       1.00      0.97      0.99        37\n",
      "              checked_fabric       0.82      0.84      0.83        58\n",
      "              chevron_fabric       0.88      0.83      0.85        53\n",
      "diagonal_grid_fabric_pattern       0.86      0.80      0.83        55\n",
      "               floral_fabric       0.71      0.62      0.66        39\n",
      "               fringe_fabric       0.91      0.76      0.83        38\n",
      "         glitch_print_fabric       0.54      0.54      0.54        35\n",
      "              glitter_fabric       0.54      0.48      0.51        29\n",
      "             gradient_fabric       0.58      0.58      0.58        31\n",
      "       graffiti_print_fabric       0.82      0.82      0.82        34\n",
      "          holographic_fabric       0.65      0.89      0.75        27\n",
      "          houndstooth_fabric       0.71      0.80      0.75        40\n",
      "  illustrative_graphic_print       0.94      0.80      0.86        20\n",
      "                 lace_fabric       0.59      0.65      0.62        26\n",
      "           leaf_print_fabric       0.82      0.69      0.75        48\n",
      "        leopard_print_fabric       0.87      0.92      0.89        50\n",
      "       marble_pattern_fabric       0.70      0.79      0.75        39\n",
      "             metallic_fabric       0.55      0.43      0.48        28\n",
      "  military_camouflage_fabric       0.92      0.92      0.92        49\n",
      "              paisley_fabric       0.60      0.83      0.69        30\n",
      "    plain_solid_color_fabric       0.60      0.76      0.67        62\n",
      "            polka_dot_fabric       0.87      0.90      0.88        68\n",
      "               sequin_fabric       0.61      0.47      0.53        43\n",
      "           snake_skin_fabric       0.82      0.85      0.84        33\n",
      "              striped_fabric       0.83      0.82      0.83        73\n",
      "              tie_dye_fabric       0.85      0.72      0.78        47\n",
      "          zebra_print_fabric       0.92      0.92      0.92        59\n",
      "\n",
      "                    accuracy                           0.77      1200\n",
      "                   macro avg       0.76      0.75      0.75      1200\n",
      "                weighted avg       0.77      0.77      0.77      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPVisionModel, CLIPProcessor, TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "import shutil\n",
    "import random\n",
    "from PIL import ImageOps, ImageEnhance\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total, used, free = shutil.disk_usage(\".\")\n",
    "print(f\"Disk space: {free // (2**20)} MB free\")\n",
    "if free < 5 * 2**30:\n",
    "    raise OSError(\"Not enough disk space. Please free up space before running this notebook.\")\n",
    "\n",
    "# Load dataset\n",
    "data_dir = \"fabric_dataset2\"\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=data_dir)\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Label mapping\n",
    "labels = split[\"train\"].features[\"label\"].names\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# CLIP processor\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Augmentation\n",
    "def apply_augmentation(image):\n",
    "    if random.random() < 0.5:\n",
    "        image = ImageOps.mirror(image)\n",
    "    if random.random() < 0.3:\n",
    "        image = image.rotate(random.choice([-15, -10, -5, 5, 10, 15]))\n",
    "    if random.random() < 0.3:\n",
    "        image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8, 1.2))\n",
    "    if random.random() < 0.3:\n",
    "        image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8, 1.2))\n",
    "    if random.random() < 0.3:\n",
    "        image = ImageOps.autocontrast(image)\n",
    "    if random.random() < 0.3:\n",
    "        image = ImageEnhance.Color(image).enhance(random.uniform(0.8, 1.2))\n",
    "    return image\n",
    "\n",
    "# Transform\n",
    "def transform_fn(example):\n",
    "    image = None\n",
    "    try:\n",
    "        image = example[\"image\"]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(np.array(image))\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = apply_augmentation(image)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        if pixel_values.shape != (3, 224, 224):\n",
    "            raise ValueError(\"Invalid pixel_values shape\")\n",
    "        return {\"pixel_values\": pixel_values, \"label\": int(example[\"label\"])}\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping image due to error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if image is not None and hasattr(image, \"close\"):\n",
    "            try:\n",
    "                image.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "split = split.map(transform_fn, remove_columns=[\"image\"], num_proc=1)\n",
    "split = split.filter(lambda x: x is not None and \"pixel_values\" in x and \"label\" in x)\n",
    "split[\"train\"] = split[\"train\"].map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "split[\"test\"] = split[\"test\"].map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "\n",
    "split[\"train\"].set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "split[\"test\"].set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "\n",
    "# --- CutMix and Mixup helpers ---\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    ''' x: images, y: labels '''\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x.size()[0])\n",
    "    y_a = y\n",
    "    y_b = y[rand_index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[rand_index, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Model\n",
    "class CLIPViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, use_cutmix=False, use_mixup=False, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        for param in self.clip_vit.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.clip_vit.config.hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.use_cutmix = use_cutmix\n",
    "        self.use_mixup = use_mixup\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # Only apply CutMix/Mixup during training and if labels are provided\n",
    "        if self.training and labels is not None:\n",
    "            if self.use_cutmix:\n",
    "                pixel_values, targets1, targets2, lam = cutmix_data(pixel_values, labels, self.alpha)\n",
    "            elif self.use_mixup:\n",
    "                pixel_values, targets1, targets2, lam = mixup_data(pixel_values, labels, self.alpha)\n",
    "            else:\n",
    "                targets1, targets2, lam = labels, labels, 1.0\n",
    "        else:\n",
    "            targets1, targets2, lam = labels, labels, 1.0\n",
    "\n",
    "        outputs = self.clip_vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.training and (self.use_cutmix or self.use_mixup):\n",
    "                loss = lam * self.loss_fn(logits, targets1) + (1 - lam) * self.loss_fn(logits, targets2)\n",
    "            else:\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits)\n",
    "\n",
    "# --- Choose one: use_cutmix=True or use_mixup=True ---\n",
    "model = CLIPViTClassifier(num_classes=len(labels), use_cutmix=True, alpha=1.0).to(device)\n",
    "# model = CLIPViTClassifier(num_classes=len(labels), use_mixup=True, alpha=1.0).to(device)\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./clip-vit-fabric2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[], \n",
    "    max_grad_norm=0.5,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    # optimizer_class=torch.optim.AdamW,  # Removed invalid argument\n",
    "    # optimizers=(None, None),  # Use default optimizer\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=split[\"train\"],\n",
    "    eval_dataset=split[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "# Evaluate\n",
    "preds = trainer.predict(split[\"test\"])\n",
    "y_true = preds.label_ids\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9e01c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"clip-vit-fabric2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b925614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "class CLIPViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.clip_vit.config.hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits)\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "model = CLIPViTClassifier(num_classes=28)  # match the checkpoint's number of classes\n",
    "state_dict = load_file(\"clip-vit-fabric2/checkpoint-3000/model.safetensors\", device=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "# Save the model in a format compatible with the Hugging Face Transformers library\n",
    "from safetensors.torch import save_file\n",
    "save_file(model.state_dict(), \"clip-vit-fabric2-hf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
