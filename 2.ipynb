{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5f417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1454205acd624df191d758dd56443fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b63aa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872824085a2443ba9da9ac3e6901f01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "dataset = load_dataset(\"yainage90/fashion-pattern-images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a7a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['argyle', 'camouflage', 'checked', 'dot', 'floral', 'geometric', 'gradient', 'graphic', 'houndstooth', 'leopard', 'lettering', 'muji', 'paisley', 'snake_skin', 'snow_flake', 'stripe', 'tropical', 'zebra', 'zigzag']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "577185f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 1520\n",
      "Testset size: 380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train set into train and test\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "trainset = split[\"train\"]\n",
    "testset = split[\"test\"]\n",
    "print(\"Trainset size:\", len(trainset))\n",
    "print(\"Testset size:\", len(testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a84eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a91043496654bbb83ab5f22db4476a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89993ccb0d7b4e72aa07280a58398114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9491, 'grad_norm': 1.5425986051559448, 'learning_rate': 9.894736842105264e-06, 'epoch': 0.11}\n",
      "{'loss': 2.9435, 'grad_norm': 1.5613164901733398, 'learning_rate': 9.789473684210527e-06, 'epoch': 0.21}\n",
      "{'loss': 2.9213, 'grad_norm': 1.6143027544021606, 'learning_rate': 9.68421052631579e-06, 'epoch': 0.32}\n",
      "{'loss': 2.9271, 'grad_norm': 1.7591242790222168, 'learning_rate': 9.578947368421054e-06, 'epoch': 0.42}\n",
      "{'loss': 2.9126, 'grad_norm': 1.7223918437957764, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.53}\n",
      "{'loss': 2.9204, 'grad_norm': 1.4843759536743164, 'learning_rate': 9.36842105263158e-06, 'epoch': 0.63}\n",
      "{'loss': 2.9241, 'grad_norm': 1.6439865827560425, 'learning_rate': 9.263157894736842e-06, 'epoch': 0.74}\n",
      "{'loss': 2.91, 'grad_norm': 1.5305286645889282, 'learning_rate': 9.157894736842105e-06, 'epoch': 0.84}\n",
      "{'loss': 2.9036, 'grad_norm': 1.703059196472168, 'learning_rate': 9.05263157894737e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e58722a5d7c41d9b0500415b1eae754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8934314250946045, 'eval_accuracy': 0.1736842105263158, 'eval_f1': 0.15399213333823342, 'eval_runtime': 126.7072, 'eval_samples_per_second': 2.999, 'eval_steps_per_second': 0.189, 'epoch': 1.0}\n",
      "{'loss': 2.8582, 'grad_norm': 1.7762049436569214, 'learning_rate': 8.947368421052632e-06, 'epoch': 1.05}\n",
      "{'loss': 2.8327, 'grad_norm': 1.7582260370254517, 'learning_rate': 8.842105263157895e-06, 'epoch': 1.16}\n",
      "{'loss': 2.8278, 'grad_norm': 1.68967866897583, 'learning_rate': 8.736842105263158e-06, 'epoch': 1.26}\n",
      "{'loss': 2.8191, 'grad_norm': 1.7700130939483643, 'learning_rate': 8.631578947368422e-06, 'epoch': 1.37}\n",
      "{'loss': 2.7909, 'grad_norm': 1.6738005876541138, 'learning_rate': 8.526315789473685e-06, 'epoch': 1.47}\n",
      "{'loss': 2.8011, 'grad_norm': 1.7791657447814941, 'learning_rate': 8.421052631578948e-06, 'epoch': 1.58}\n",
      "{'loss': 2.7644, 'grad_norm': 1.8091455698013306, 'learning_rate': 8.315789473684212e-06, 'epoch': 1.68}\n",
      "{'loss': 2.7703, 'grad_norm': 1.8883768320083618, 'learning_rate': 8.210526315789475e-06, 'epoch': 1.79}\n",
      "{'loss': 2.773, 'grad_norm': 1.7720739841461182, 'learning_rate': 8.105263157894736e-06, 'epoch': 1.89}\n",
      "{'loss': 2.7444, 'grad_norm': 1.866727352142334, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec0b8e357e042408786827d8f40e450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8095755577087402, 'eval_accuracy': 0.2894736842105263, 'eval_f1': 0.25121845488089367, 'eval_runtime': 124.641, 'eval_samples_per_second': 3.049, 'eval_steps_per_second': 0.193, 'epoch': 2.0}\n",
      "{'loss': 2.6931, 'grad_norm': 1.941855788230896, 'learning_rate': 7.894736842105265e-06, 'epoch': 2.11}\n",
      "{'loss': 2.6998, 'grad_norm': 1.9155468940734863, 'learning_rate': 7.789473684210526e-06, 'epoch': 2.21}\n",
      "{'loss': 2.6807, 'grad_norm': 1.9378896951675415, 'learning_rate': 7.68421052631579e-06, 'epoch': 2.32}\n",
      "{'loss': 2.645, 'grad_norm': 1.9952360391616821, 'learning_rate': 7.578947368421054e-06, 'epoch': 2.42}\n",
      "{'loss': 2.6384, 'grad_norm': 1.9649302959442139, 'learning_rate': 7.473684210526316e-06, 'epoch': 2.53}\n",
      "{'loss': 2.6132, 'grad_norm': 1.9654693603515625, 'learning_rate': 7.368421052631579e-06, 'epoch': 2.63}\n",
      "{'loss': 2.6476, 'grad_norm': 2.2477834224700928, 'learning_rate': 7.263157894736843e-06, 'epoch': 2.74}\n",
      "{'loss': 2.6108, 'grad_norm': 1.9668926000595093, 'learning_rate': 7.157894736842106e-06, 'epoch': 2.84}\n",
      "{'loss': 2.6373, 'grad_norm': 2.09828782081604, 'learning_rate': 7.052631578947369e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124f1900dd45474883eaee521e3d9e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.713651418685913, 'eval_accuracy': 0.3815789473684211, 'eval_f1': 0.3567176872269258, 'eval_runtime': 69.4733, 'eval_samples_per_second': 5.47, 'eval_steps_per_second': 0.345, 'epoch': 3.0}\n",
      "{'loss': 2.5511, 'grad_norm': 1.953678011894226, 'learning_rate': 6.947368421052632e-06, 'epoch': 3.05}\n",
      "{'loss': 2.549, 'grad_norm': 2.0125861167907715, 'learning_rate': 6.842105263157896e-06, 'epoch': 3.16}\n",
      "{'loss': 2.519, 'grad_norm': 1.9681227207183838, 'learning_rate': 6.736842105263158e-06, 'epoch': 3.26}\n",
      "{'loss': 2.527, 'grad_norm': 2.1301472187042236, 'learning_rate': 6.631578947368421e-06, 'epoch': 3.37}\n",
      "{'loss': 2.5103, 'grad_norm': 2.097996234893799, 'learning_rate': 6.526315789473685e-06, 'epoch': 3.47}\n",
      "{'loss': 2.4873, 'grad_norm': 2.1301376819610596, 'learning_rate': 6.421052631578948e-06, 'epoch': 3.58}\n",
      "{'loss': 2.5112, 'grad_norm': 2.075584650039673, 'learning_rate': 6.31578947368421e-06, 'epoch': 3.68}\n",
      "{'loss': 2.471, 'grad_norm': 2.2883036136627197, 'learning_rate': 6.2105263157894745e-06, 'epoch': 3.79}\n",
      "{'loss': 2.4468, 'grad_norm': 2.1853244304656982, 'learning_rate': 6.105263157894738e-06, 'epoch': 3.89}\n",
      "{'loss': 2.4594, 'grad_norm': 1.9579566717147827, 'learning_rate': 6e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a9b0655f8b490a81b28a5bac2bc5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.611682653427124, 'eval_accuracy': 0.4789473684210526, 'eval_f1': 0.4624292456146372, 'eval_runtime': 125.613, 'eval_samples_per_second': 3.025, 'eval_steps_per_second': 0.191, 'epoch': 4.0}\n",
      "{'loss': 2.3912, 'grad_norm': 2.0000555515289307, 'learning_rate': 5.8947368421052634e-06, 'epoch': 4.11}\n",
      "{'loss': 2.3837, 'grad_norm': 2.0142300128936768, 'learning_rate': 5.789473684210527e-06, 'epoch': 4.21}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     88\u001b[39m trainer = Trainer(\n\u001b[32m     89\u001b[39m     model=model,\n\u001b[32m     90\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     96\u001b[39m )\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    102\u001b[39m metrics = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2284\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m   2279\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs)\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load dataset and split\n",
    "dataset = load_dataset(\"yainage90/fashion-pattern-images\")\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "trainset = split[\"train\"]\n",
    "testset = split[\"test\"]\n",
    "\n",
    "# Get label info\n",
    "labels = trainset.features[\"label\"].names\n",
    "num_labels = len(labels)\n",
    "id2label = {str(i): l for i, l in enumerate(labels)}\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "# Preprocessing (ViT processor + optional augmentation)\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "def transform_examples(batch):\n",
    "    # Data augmentation only on training set\n",
    "    if \"train\" in batch[\"__split__\"][0]:\n",
    "        images = [augment(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    else:\n",
    "        images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    processed = processor(images=images, return_tensors=\"pt\")\n",
    "    # Remove batch dimension for each image\n",
    "    pixel_values = [img for img in processed[\"pixel_values\"]]\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": batch[\"label\"]\n",
    "    }\n",
    "\n",
    "# Add split info for augmentation\n",
    "trainset = trainset.add_column(\"__split__\", [\"train\"] * len(trainset))\n",
    "testset = testset.add_column(\"__split__\", [\"test\"] * len(testset))\n",
    "\n",
    "trainset = trainset.map(transform_examples, batched=True, remove_columns=trainset.column_names)\n",
    "testset = testset.map(transform_examples, batched=True, remove_columns=testset.column_names)\n",
    "\n",
    "# Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(x[\"pixel_values\"]) for x in batch])\n",
    "    labels = torch.tensor([x[\"labels\"] for x in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-pattern\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,  # Try more epochs for better results\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=8,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"none\",\n",
    "    learning_rate=1e-5,  # Lower learning rate for better fine-tuning\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=None,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99271a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
